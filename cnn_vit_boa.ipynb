{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac539f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage.feature import hog, local_binary_pattern\n",
    "from skimage import color\n",
    "\n",
    "BASE_DIR = \"cifakemini\"  \n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 15\n",
    "NUM_CLASSES = 2  \n",
    "IMG_SIZE = 224\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "])\n",
    "\n",
    "def extract_hog_features(image):\n",
    "    image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    fd, hog_image = hog(image_gray, pixels_per_cell=(8, 8), cells_per_block=(2, 2), visualize=True)\n",
    "    return fd\n",
    "\n",
    "def extract_lbp_features(image):\n",
    "    image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    lbp = local_binary_pattern(image_gray, P=8, R=1, method='uniform')\n",
    "    return lbp.ravel()\n",
    "\n",
    "def extract_color_moments(image):\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    moments = []\n",
    "    for i in range(3):  \n",
    "        channel = image_rgb[:, :, i]\n",
    "        mean = np.mean(channel)\n",
    "        std = np.std(channel)\n",
    "        \n",
    "        skew = np.mean((channel - mean) ** 3) / (std ** 3) if std != 0 else 0\n",
    "        moments.extend([mean, std, skew])\n",
    "    return moments\n",
    "\n",
    "\n",
    "class CustomImageFolder(ImageFolder):\n",
    "    def __getitem__(self, index):\n",
    "        img_path, label = self.imgs[index]\n",
    "        pil_img = self.loader(img_path)\n",
    "        img_tensor = self.transform(pil_img)\n",
    "\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "           \n",
    "            dummy_hog = extract_hog_features(np.zeros((224, 224, 3), dtype=np.uint8))\n",
    "            dummy_lbp = extract_lbp_features(np.zeros((224, 224, 3), dtype=np.uint8))\n",
    "            dummy_color = extract_color_moments(np.zeros((224, 224, 3), dtype=np.uint8))\n",
    "\n",
    "            hog_f = np.zeros_like(dummy_hog)\n",
    "            lbp_f = np.zeros_like(dummy_lbp)\n",
    "            color_f = np.zeros_like(dummy_color)\n",
    "        else:\n",
    "            hog_f = extract_hog_features(image)\n",
    "            lbp_f = extract_lbp_features(image)\n",
    "            color_f = extract_color_moments(image)\n",
    "\n",
    "        handcrafted = np.concatenate([hog_f, lbp_f, color_f])\n",
    "        handcrafted = torch.tensor(handcrafted, dtype=torch.float32)\n",
    "\n",
    "        return img_tensor, handcrafted, label\n",
    "\n",
    "\n",
    "train_dataset = CustomImageFolder(root=os.path.join(BASE_DIR, \"train\"), transform=transform)\n",
    "val_dataset = CustomImageFolder(root=os.path.join(BASE_DIR, \"test\"), transform=transform)\n",
    "test_dataset = CustomImageFolder(root=os.path.join(BASE_DIR, \"test\"), transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "_, sample_handcrafted, _ = next(iter(train_loader))\n",
    "handcrafted_dim = sample_handcrafted.shape[1]\n",
    "\n",
    "print(f\"Actual handcrafted dimension from DataLoader: {handcrafted_dim}\")\n",
    "\n",
    "class CNNViT(nn.Module):\n",
    "    def __init__(self, num_classes=2, handcrafted_dim=0):\n",
    "        super(CNNViT, self).__init__()\n",
    "        self.cnn = models.resnet18(pretrained=True)\n",
    "        self.cnn = nn.Sequential(*list(self.cnn.children())[:-2])  \n",
    "        self.vit = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=0)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        cnn_out_dim = 512  \n",
    "        vit_out_dim = self.vit.num_features  \n",
    "\n",
    "       \n",
    "        print(f\"CNN output dim: {cnn_out_dim}\")\n",
    "        print(f\"ViT output dim: {vit_out_dim}\")\n",
    "        print(f\"Handcrafted features dim: {handcrafted_dim}\")\n",
    "\n",
    "        total_feature_dim = cnn_out_dim + vit_out_dim + handcrafted_dim\n",
    "        print(f\"Total feature dim: {total_feature_dim}\")\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(total_feature_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)  \n",
    "        )\n",
    "\n",
    "    def forward(self, x, handcrafted=None):\n",
    "   \n",
    "        cnn_feat = self.cnn(x)\n",
    "        cnn_feat = self.avgpool(cnn_feat)\n",
    "        cnn_feat = cnn_feat.view(cnn_feat.size(0), -1)  \n",
    "\n",
    "       \n",
    "        vit_feat = self.vit(x)  \n",
    "     \n",
    "        if handcrafted is not None:\n",
    "            combined = torch.cat((cnn_feat, vit_feat, handcrafted), dim=1)\n",
    "        else:\n",
    "            combined = torch.cat((cnn_feat, vit_feat), dim=1)\n",
    "\n",
    "        return self.fc(combined)\n",
    "\n",
    "\n",
    "def evaluate(loader, model):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, handcrafted, labels in loader:\n",
    "            imgs, handcrafted, labels = imgs.to(DEVICE), handcrafted.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(imgs, handcrafted)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    prec = precision_score(all_labels, all_preds, average='weighted')\n",
    "    rec = recall_score(all_labels, all_labels, average='weighted') \n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "def boa_optimize(model_class, train_loader, val_loader, population_size=5, iterations=5):\n",
    "    lb, ub = 1e-6, 1e-3  \n",
    "\n",
    "    \n",
    "    population = np.random.uniform(lb, ub, population_size)\n",
    "    fitness = np.zeros(population_size)\n",
    "\n",
    "    for i in range(population_size):\n",
    "        lr = population[i]\n",
    "        \n",
    "        model = model_class(num_classes=NUM_CLASSES, handcrafted_dim=handcrafted_dim).to(DEVICE)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "      \n",
    "        model.train()\n",
    "        imgs, handcrafted, labels = next(iter(train_loader))\n",
    "        imgs, handcrafted, labels = imgs.to(DEVICE), handcrafted.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs, handcrafted)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, _, _, f1 = evaluate(val_loader, model)\n",
    "        fitness[i] = f1\n",
    "\n",
    "    best_lr = population[np.argmax(fitness)]\n",
    "    best_f1 = np.max(fitness)\n",
    "\n",
    "   \n",
    "    for t in range(iterations):\n",
    "        fmin = np.min(fitness)\n",
    "        fmax = np.max(fitness)\n",
    "\n",
    "        for i in range(population_size):\n",
    "            r = np.random.rand()\n",
    "            power_exponent = 0.5  \n",
    "            a = 0.1              \n",
    "\n",
    "            fragrance = a * (fitness[i]**power_exponent)\n",
    "\n",
    "            if r < 0.8:  \n",
    "                j = np.argmax(fitness)\n",
    "                population[i] = population[i] + fragrance * (fitness[j] - fitness[i]) * np.random.rand()\n",
    "            else:  \n",
    "                eps = np.random.uniform(-1, 1)\n",
    "                k = np.random.randint(population_size)\n",
    "                l = np.random.randint(population_size)\n",
    "                population[i] = population[i] + eps * (fitness[k] - fitness[l]) * fragrance\n",
    "\n",
    "            \n",
    "            population[i] = np.clip(population[i], lb, ub)\n",
    "\n",
    "            \n",
    "            lr = population[i]\n",
    "            \n",
    "            model = model_class(num_classes=NUM_CLASSES, handcrafted_dim=handcrafted_dim).to(DEVICE)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            imgs, handcrafted, labels = next(iter(train_loader))\n",
    "            imgs, handcrafted, labels = imgs.to(DEVICE), handcrafted.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs, handcrafted)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, _, _, f1 = evaluate(val_loader, model)\n",
    "            fitness[i] = f1\n",
    "\n",
    "        if np.max(fitness) > best_f1:\n",
    "            best_f1 = np.max(fitness)\n",
    "            best_lr = population[np.argmax(fitness)]\n",
    "\n",
    "    return best_lr\n",
    "\n",
    "\n",
    "print(\"Starting BOA for learning rate tuning...\")\n",
    "best_lr = boa_optimize(CNNViT, train_loader, val_loader)\n",
    "print(f\"\\ud83d\\udccc Best LR from BOA: {best_lr:.8f}\")\n",
    "\n",
    "\n",
    "model = CNNViT(num_classes=NUM_CLASSES, handcrafted_dim=handcrafted_dim).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=best_lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    all_preds, all_labels = [], []\n",
    "    epoch_loss = 0\n",
    "    for imgs, handcrafted, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        imgs, handcrafted, labels = imgs.to(DEVICE), handcrafted.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs, handcrafted)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        all_preds.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    train_acc = accuracy_score(all_labels, all_preds)\n",
    "    val_acc, val_prec, val_rec, val_f1 = evaluate(val_loader, model)\n",
    "\n",
    "    print(f\" Epoch {epoch+1} | Loss: {epoch_loss:.4f} | Train Acc: {train_acc*100:.2f}% | \"\n",
    "          f\"Val Acc: {val_acc*100:.2f}% | Precision: {val_prec:.2f} | Recall: {val_rec:.2f} | F1: {val_f1:.2f}\")\n",
    "\n",
    "\n",
    "test_acc, test_prec, test_rec, test_f1 = evaluate(test_loader, model)\n",
    "print(f\"\\n Final Test Accuracy: {test_acc*100:.2f}% | Precision: {test_prec:.2f} | Recall: {test_rec:.2f} | F1-Score: {test_f1:.2f}\")\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), \"cnn_vit_classifier.pth\")\n",
    "print(\" Model saved as cnn_vit_classifier.pth\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
